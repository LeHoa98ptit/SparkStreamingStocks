{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51fcb082",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.9/dist-packages/pyspark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-a4acf3e4-c8a6-4058-a05b-fd3d3c400cb0;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.0.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.4.1 in central\n",
      "\tfound com.github.luben#zstd-jni;1.4.4-3 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.7.5 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      ":: resolution report :: resolve 258ms :: artifacts dl 4ms\n",
      "\t:: modules in use:\n",
      "\tcom.github.luben#zstd-jni;1.4.4-3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.0.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.7.5 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   9   |   0   |   0   |   0   ||   9   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-a4acf3e4-c8a6-4058-a05b-fd3d3c400cb0\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 9 already retrieved (0kB/4ms)\n",
      "23/11/06 20:00:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/06 20:00:22 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/11/06 20:00:22 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyApp\") \\\n",
    "    .config(\"spark.jars.packages\", 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0') \\\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99e59fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, BooleanType, TimestampType, DateType\n",
    "\n",
    "schema = StructType(\n",
    "      [\n",
    "        StructField(\"name\", StringType(), False),\n",
    "        StructField(\"price\", DoubleType(), False),\n",
    "        StructField(\"timestamp\", TimestampType(), False),\n",
    "      ]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0aa8086e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_server = \"kafka1:9092\"   \n",
    "from pyspark.sql.functions import from_json\n",
    "\n",
    "lines = (spark.readStream                        # Get the DataStreamReader\n",
    "  .format(\"kafka\")                                 # Specify the source format as \"kafka\"\n",
    "  .option(\"kafka.bootstrap.servers\", kafka_server) # Configure the Kafka server name and port\n",
    "  .option(\"subscribe\", \"stock\")                       # Subscribe to the \"en\" Kafka topic \n",
    "  .option(\"startingOffsets\", \"earliest\")           # The start point when a query is started\n",
    "  .option(\"maxOffsetsPerTrigger\", 100)             # Rate limit on max offsets per trigger interval\n",
    "  .load()\n",
    "  .select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"parsed_value\"))\n",
    "# Load the DataFrame\n",
    ")\n",
    "df = lines.select(\"parsed_value.*\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933b581e",
   "metadata": {},
   "source": [
    "## The assignment starts here\n",
    "\n",
    "You can create a\n",
    "\n",
    "## Select the stocks that lost value between two windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d04e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/06 20:00:30 WARN StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-6d700f1c-cc93-4c37-bf51-638a7339d738. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "[Stage 3:======================================================>(197 + 3) / 200]\r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lag, window\n",
    "\n",
    "# The function processes each batch of data\n",
    "def process_batch(df, epoch_id):\n",
    "    \n",
    "    window_spec = Window.partitionBy(\"name\").orderBy(\"window\")\n",
    "\n",
    "    # Create a column \"previous_window\" and \"previous_price\" using the lag function\n",
    "    df = df.withColumn(\"previous_price\", lag(\"avg(price)\").over(window_spec))\n",
    "    df = df.withColumn(\"previous_window\", lag(\"window\").over(window_spec))\n",
    "    df = df.filter(df[\"previous_price\"] > df[\"avg(price)\"])\n",
    "    \n",
    "    # Show data\n",
    "    df.show(truncate=False)\n",
    "    \n",
    "\n",
    "# Apply a time window to the data with a watermark of 30 seconds\n",
    "# Group the data by a 5-minute window and the stock name\n",
    "# Calculate the average price within each window for each stock\n",
    "\n",
    "windowedDF_2 = df \\\n",
    "        .withWatermark(\"timestamp\", \"30 seconds\") \\\n",
    "        .groupBy(window(\"timestamp\", \"5 minutes\"), \"name\") \\\n",
    "        .agg({\"price\": \"avg\"})\n",
    "\n",
    "# Order the results by average price in descending order \n",
    "lost_value_stocks = windowedDF_2.orderBy(\"avg(price)\", ascending=False)\n",
    "\n",
    "\n",
    "# Apply function process_batch, save and show result\n",
    "query_2 = (lost_value_stocks.writeStream\n",
    "           .outputMode(\"complete\")\n",
    "           .format(\"memory\")\n",
    "           .queryName(\"TheStocksThatLostValue1\")\n",
    "           .option(\"truncate\", False)\n",
    "           .foreachBatch(process_batch)\n",
    "           .start())\n",
    "    \n",
    "query_2.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0066105b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
